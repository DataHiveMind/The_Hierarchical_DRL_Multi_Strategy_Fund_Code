{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45eb5169",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis Notebook\n",
    "-   This Notebook is to load raw market data, clean it and explore it and add features to the data for spealist to train on.\n",
    "\n",
    "## Steps: \n",
    "    1.  Download Raw Market \n",
    "    2.  Clean the Data\n",
    "    3.  Add Features to the data\n",
    "    4.  Visulaze the data\n",
    "    5.  Save the Cleaned Data and Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc64b0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Configure warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add parent directory to path\n",
    "sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath('__file__'))))\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8faa9c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import custom modules\n",
    "from src.data.data_loader import DataLoader\n",
    "from src.data.feature_engineering import FeatureEngineer\n",
    "\n",
    "print(\"Custom modules imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d45a42f",
   "metadata": {},
   "source": [
    "## Step 1: Downlaod Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e154166f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define asset universes for each portfolio\n",
    "# Date range for historical data\n",
    "start_date = '2020-01-01'\n",
    "end_date = '2024-12-31'\n",
    "\n",
    "# EQUITIES PORTFOLIO - Liquid US stocks across sectors\n",
    "equities_tickers = [\n",
    "    'AAPL', 'MSFT', 'GOOGL', 'AMZN', 'NVDA',  # Tech\n",
    "    'JPM', 'BAC', 'GS', 'MS', 'C',             # Financials\n",
    "    'JNJ', 'UNH', 'PFE', 'ABBV', 'TMO',        # Healthcare\n",
    "    'XOM', 'CVX', 'COP', 'SLB', 'EOG',         # Energy\n",
    "    'WMT', 'HD', 'MCD', 'NKE', 'COST'          # Consumer\n",
    "]\n",
    "\n",
    "# FX PORTFOLIO - Major currency pairs (using FX futures/ETFs as proxy)\n",
    "fx_tickers = [\n",
    "    'EURUSD=X',  # EUR/USD\n",
    "    'GBPUSD=X',  # GBP/USD\n",
    "    'USDJPY=X',  # USD/JPY\n",
    "    'AUDUSD=X',  # AUD/USD\n",
    "    'USDCAD=X',  # USD/CAD\n",
    "    'USDCHF=X',  # USD/CHF\n",
    "    'NZDUSD=X',  # NZD/USD\n",
    "    'EURGBP=X',  # EUR/GBP\n",
    "    'EURJPY=X',  # EUR/JPY\n",
    "    'GBPJPY=X'   # GBP/JPY\n",
    "]\n",
    "\n",
    "# FUTURES PORTFOLIO - Using ETFs as proxies for futures contracts\n",
    "futures_tickers = [\n",
    "    'GC=F',   # Gold Futures\n",
    "    'SI=F',   # Silver Futures\n",
    "    'CL=F',   # Crude Oil WTI Futures\n",
    "    'NG=F',   # Natural Gas Futures\n",
    "    'ZC=F',   # Corn Futures\n",
    "    'ZW=F',   # Wheat Futures\n",
    "    'HG=F',   # Copper Futures\n",
    "    'ES=F',   # E-mini S&P 500 Futures\n",
    "    'NQ=F',   # E-mini NASDAQ Futures\n",
    "    'ZB=F'    # Treasury Bond Futures\n",
    "]\n",
    "\n",
    "print(f\"Data range: {start_date} to {end_date}\")\n",
    "print(f\"\\nEquities Portfolio: {len(equities_tickers)} tickers\")\n",
    "print(f\"FX Portfolio: {len(fx_tickers)} pairs\")\n",
    "print(f\"Futures Portfolio: {len(futures_tickers)} contracts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5054aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download data for each portfolio with separate ArcticDB databases\n",
    "print(\"=\" * 80)\n",
    "print(\"DOWNLOADING EQUITIES DATA\")\n",
    "print(\"=\" * 80)\n",
    "equities_loader = DataLoader(equities_tickers, start_date, end_date, interval='1d', \n",
    "                             db_url='lmdb://equities_data')\n",
    "equities_raw = equities_loader.load_raw_data()\n",
    "print(f\"Equities data shape: {equities_raw.shape}\")\n",
    "print(f\"Equities columns: {equities_raw.columns.tolist()[:5]}\")  # Show first 5 columns\n",
    "if 'Date' in equities_raw.columns:\n",
    "    print(f\"Equities date range: {equities_raw['Date'].min()} to {equities_raw['Date'].max()}\")\n",
    "else:\n",
    "    print(f\"Equities index range: {equities_raw.index.min()} to {equities_raw.index.max()}\")\n",
    "print(f\"Sample data:\\n{equities_raw.head(3)}\")\n",
    "\n",
    "# Store in ArcticDB\n",
    "print(\"\\nStoring equities data in ArcticDB (equities_data database)...\")\n",
    "equities_loader.store_data(\"raw_market_data\", equities_raw)\n",
    "print(\"âœ… Equities data stored in ArcticDB\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DOWNLOADING FX DATA\")\n",
    "print(\"=\" * 80)\n",
    "fx_loader = DataLoader(fx_tickers, start_date, end_date, interval='1d',\n",
    "                       db_url='lmdb://fx_data')\n",
    "fx_raw = fx_loader.load_raw_data()\n",
    "print(f\"FX data shape: {fx_raw.shape}\")\n",
    "print(f\"FX columns: {fx_raw.columns.tolist()[:5]}\")\n",
    "if 'Date' in fx_raw.columns:\n",
    "    print(f\"FX date range: {fx_raw['Date'].min()} to {fx_raw['Date'].max()}\")\n",
    "else:\n",
    "    print(f\"FX index range: {fx_raw.index.min()} to {fx_raw.index.max()}\")\n",
    "print(f\"Sample data:\\n{fx_raw.head(3)}\")\n",
    "\n",
    "# Store in ArcticDB\n",
    "print(\"\\nStoring FX data in ArcticDB (fx_data database)...\")\n",
    "fx_loader.store_data(\"raw_market_data\", fx_raw)\n",
    "print(\"âœ… FX data stored in ArcticDB\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DOWNLOADING FUTURES DATA\")\n",
    "print(\"=\" * 80)\n",
    "futures_loader = DataLoader(futures_tickers, start_date, end_date, interval='1d',\n",
    "                            db_url='lmdb://futures_data')\n",
    "futures_raw = futures_loader.load_raw_data()\n",
    "print(f\"Futures data shape: {futures_raw.shape}\")\n",
    "print(f\"Futures columns: {futures_raw.columns.tolist()[:5]}\")\n",
    "if 'Date' in futures_raw.columns:\n",
    "    print(f\"Futures date range: {futures_raw['Date'].min()} to {futures_raw['Date'].max()}\")\n",
    "else:\n",
    "    print(f\"Futures index range: {futures_raw.index.min()} to {futures_raw.index.max()}\")\n",
    "print(f\"Sample data:\\n{futures_raw.head(3)}\")\n",
    "\n",
    "# Store in ArcticDB\n",
    "print(\"\\nStoring futures data in ArcticDB (futures_data database)...\")\n",
    "futures_loader.store_data(\"raw_market_data\", futures_raw)\n",
    "print(\"âœ… Futures data stored in ArcticDB\")\n",
    "\n",
    "print(\"\\nâœ… All data downloaded and stored in separate ArcticDB databases!\")\n",
    "print(\"\\nğŸ“ ArcticDB Databases Created:\")\n",
    "print(\"   - lmdb://equities_data\")\n",
    "print(\"   - lmdb://fx_data\")\n",
    "print(\"   - lmdb://futures_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df1d0a8",
   "metadata": {},
   "source": [
    "## Step 2: Clean the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8904ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data cleaning and quality checks\n",
    "def clean_and_analyze_data(df, portfolio_name):\n",
    "    \"\"\"Clean data and provide quality metrics.\"\"\"\n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(f\"{portfolio_name.upper()} - DATA QUALITY ANALYSIS\")\n",
    "    print(f\"{'=' * 80}\")\n",
    "    \n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # Set Date as index\n",
    "    if 'Date' in df_clean.columns:\n",
    "        df_clean.set_index('Date', inplace=True)\n",
    "    \n",
    "    # Check for missing values\n",
    "    print(f\"\\nğŸ“Š Missing Values:\")\n",
    "    missing = df_clean.isnull().sum()\n",
    "    missing_pct = (missing / len(df_clean)) * 100\n",
    "    missing_df = pd.DataFrame({\n",
    "        'Missing Count': missing[missing > 0],\n",
    "        'Percentage': missing_pct[missing > 0]\n",
    "    })\n",
    "    if len(missing_df) > 0:\n",
    "        print(missing_df.head(10))\n",
    "    else:\n",
    "        print(\"No missing values found!\")\n",
    "    \n",
    "    # Handle missing values\n",
    "    df_clean.ffill(inplace=True)\n",
    "    df_clean.bfill(inplace=True)\n",
    "    \n",
    "    # Check for duplicates\n",
    "    duplicates = df_clean.index.duplicated().sum()\n",
    "    print(f\"\\nğŸ“Š Duplicate rows: {duplicates}\")\n",
    "    if duplicates > 0:\n",
    "        df_clean = df_clean[~df_clean.index.duplicated(keep='first')]\n",
    "    \n",
    "    # Data statistics\n",
    "    print(f\"\\nğŸ“Š Data Summary:\")\n",
    "    print(f\"Total rows: {len(df_clean):,}\")\n",
    "    print(f\"Total columns: {len(df_clean.columns):,}\")\n",
    "    print(f\"Memory usage: {df_clean.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "# Clean all portfolios\n",
    "equities_clean = clean_and_analyze_data(equities_raw, \"Equities\")\n",
    "fx_clean = clean_and_analyze_data(fx_raw, \"FX\")\n",
    "futures_clean = clean_and_analyze_data(futures_raw, \"Futures\")\n",
    "\n",
    "print(\"\\nâœ… All data cleaned successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d37173",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample data for each portfolio\n",
    "print(\"EQUITIES PORTFOLIO - Sample Data:\")\n",
    "print(equities_clean.head())\n",
    "print(f\"\\nColumns: {list(equities_clean.columns)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"FX PORTFOLIO - Sample Data:\")\n",
    "print(fx_clean.head())\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"FUTURES PORTFOLIO - Sample Data:\")\n",
    "print(futures_clean.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3073568",
   "metadata": {},
   "source": [
    "## Step 3: Add Features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a8116b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering for each asset\n",
    "# Note: We'll process one ticker at a time due to MultiIndex structure\n",
    "\n",
    "def engineer_features_for_portfolio(df_raw, tickers, portfolio_name):\n",
    "    \"\"\"Engineer features for each ticker in the portfolio.\"\"\"\n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(f\"ENGINEERING FEATURES FOR {portfolio_name.upper()}\")\n",
    "    print(f\"{'=' * 80}\")\n",
    "    \n",
    "    all_features = {}\n",
    "    \n",
    "    for ticker in tickers:\n",
    "        try:\n",
    "            print(f\"\\nProcessing {ticker}...\")\n",
    "            \n",
    "            # Extract single ticker data\n",
    "            ticker_df = pd.DataFrame()\n",
    "            ticker_df['Date'] = df_raw.index if isinstance(df_raw.index, pd.DatetimeIndex) else df_raw.index\n",
    "            \n",
    "            # Handle MultiIndex columns from yfinance\n",
    "            if isinstance(df_raw.columns, pd.MultiIndex):\n",
    "                for col in ['Open', 'High', 'Low', 'Close', 'Volume', 'Adj Close']:\n",
    "                    if (col, ticker) in df_raw.columns:\n",
    "                        ticker_df[col] = df_raw[(col, ticker)]\n",
    "                    elif col in df_raw.columns:\n",
    "                        ticker_df[col] = df_raw[col]\n",
    "            else:\n",
    "                # Single ticker or flattened structure\n",
    "                close_cols = [c for c in df_raw.columns if 'Close' in str(c) and ticker in str(c)]\n",
    "                if close_cols:\n",
    "                    ticker_df['Close'] = df_raw[close_cols[0]]\n",
    "                elif 'Close' in df_raw.columns:\n",
    "                    ticker_df['Close'] = df_raw['Close']\n",
    "            \n",
    "            if 'Close' not in ticker_df.columns or ticker_df['Close'].isnull().all():\n",
    "                print(f\"  âš ï¸  Skipping {ticker} - no valid Close data\")\n",
    "                continue\n",
    "            \n",
    "            # Set Date as index\n",
    "            if 'Date' in ticker_df.columns:\n",
    "                ticker_df.set_index('Date', inplace=True)\n",
    "            \n",
    "            # Engineer features\n",
    "            engineer = FeatureEngineer(ticker_df, price_col='Close')\n",
    "            ticker_features = engineer.add_all_features(\n",
    "                include_volume='Volume' in ticker_df.columns,\n",
    "                include_microstructure=True,\n",
    "                include_regime=True\n",
    "            )\n",
    "            \n",
    "            all_features[ticker] = ticker_features\n",
    "            print(f\"  âœ… {ticker}: {len(ticker_features.columns)} features created\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  âŒ Error processing {ticker}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\nâœ… Feature engineering complete for {portfolio_name}\")\n",
    "    print(f\"Successfully processed {len(all_features)}/{len(tickers)} tickers\")\n",
    "    \n",
    "    return all_features\n",
    "\n",
    "# This will take a few minutes...\n",
    "print(\"Starting feature engineering (this may take a few minutes)...\")\n",
    "equities_features = engineer_features_for_portfolio(equities_clean, equities_tickers, \"Equities\")\n",
    "fx_features = engineer_features_for_portfolio(fx_clean, fx_tickers, \"FX\")\n",
    "futures_features = engineer_features_for_portfolio(futures_clean, futures_tickers, \"Futures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd2ab5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display feature summary for sample tickers\n",
    "print(\"SAMPLE FEATURE SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if equities_features:\n",
    "    sample_ticker = list(equities_features.keys())[0]\n",
    "    print(f\"\\n{sample_ticker} (Equities) - Feature Columns:\")\n",
    "    print(equities_features[sample_ticker].columns.tolist()[:20], \"... (showing first 20)\")\n",
    "    print(f\"\\nTotal features: {len(equities_features[sample_ticker].columns)}\")\n",
    "    print(f\"\\nSample data:\")\n",
    "    print(equities_features[sample_ticker].tail(3))\n",
    "\n",
    "if fx_features:\n",
    "    sample_fx = list(fx_features.keys())[0]\n",
    "    print(f\"\\n\\n{sample_fx} (FX) - Total features: {len(fx_features[sample_fx].columns)}\")\n",
    "\n",
    "if futures_features:\n",
    "    sample_fut = list(futures_features.keys())[0]\n",
    "    print(f\"{sample_fut} (Futures) - Total features: {len(futures_features[sample_fut].columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83045aef",
   "metadata": {},
   "source": [
    "## Step 4: Visualia Data with Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5045b94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 1: Price trends for sample assets\n",
    "fig, axes = plt.subplots(3, 1, figsize=(15, 12))\n",
    "fig.suptitle('Price Trends Across Asset Classes', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Equities\n",
    "if equities_features:\n",
    "    sample_equities = list(equities_features.keys())[:3]\n",
    "    for ticker in sample_equities:\n",
    "        axes[0].plot(equities_features[ticker].index, \n",
    "                    equities_features[ticker]['Close'], \n",
    "                    label=ticker, linewidth=2)\n",
    "    axes[0].set_title('Equities Portfolio - Price Trends', fontsize=14)\n",
    "    axes[0].set_ylabel('Price ($)', fontsize=12)\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# FX\n",
    "if fx_features:\n",
    "    sample_fx = list(fx_features.keys())[:3]\n",
    "    for ticker in sample_fx:\n",
    "        axes[1].plot(fx_features[ticker].index, \n",
    "                    fx_features[ticker]['Close'], \n",
    "                    label=ticker, linewidth=2)\n",
    "    axes[1].set_title('FX Portfolio - Exchange Rate Trends', fontsize=14)\n",
    "    axes[1].set_ylabel('Exchange Rate', fontsize=12)\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Futures\n",
    "if futures_features:\n",
    "    sample_futures = list(futures_features.keys())[:3]\n",
    "    for ticker in sample_futures:\n",
    "        axes[2].plot(futures_features[ticker].index, \n",
    "                    futures_features[ticker]['Close'], \n",
    "                    label=ticker, linewidth=2)\n",
    "    axes[2].set_title('Futures Portfolio - Price Trends', fontsize=14)\n",
    "    axes[2].set_ylabel('Price', fontsize=12)\n",
    "    axes[2].set_xlabel('Date', fontsize=12)\n",
    "    axes[2].legend()\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/figures/price_trends.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… Price trends plot saved to reports/figures/price_trends.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c862349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization 2: Returns and Volatility Analysis\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "fig.suptitle('Returns and Volatility Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "def plot_returns_analysis(features_dict, ax_return, ax_vol, title):\n",
    "    \"\"\"Plot returns distribution and volatility for a portfolio.\"\"\"\n",
    "    all_returns = []\n",
    "    all_vols = []\n",
    "    tickers = []\n",
    "    \n",
    "    for ticker, df in features_dict.items():\n",
    "        if 'return_1d' in df.columns and 'volatility_20d' in df.columns:\n",
    "            returns = df['return_1d'].dropna()\n",
    "            vol = df['volatility_20d'].dropna()\n",
    "            \n",
    "            all_returns.extend(returns.values)\n",
    "            if len(vol) > 0:\n",
    "                all_vols.append(vol.iloc[-1])\n",
    "                tickers.append(ticker)\n",
    "    \n",
    "    # Returns distribution\n",
    "    if all_returns:\n",
    "        ax_return.hist(all_returns, bins=50, alpha=0.7, edgecolor='black')\n",
    "        ax_return.axvline(x=0, color='red', linestyle='--', linewidth=2)\n",
    "        ax_return.set_title(f'{title} - Daily Returns Distribution', fontsize=12)\n",
    "        ax_return.set_xlabel('Daily Returns')\n",
    "        ax_return.set_ylabel('Frequency')\n",
    "        ax_return.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Volatility comparison\n",
    "    if all_vols and tickers:\n",
    "        colors = plt.cm.viridis(np.linspace(0, 1, len(tickers)))\n",
    "        ax_vol.bar(range(len(tickers)), all_vols, color=colors)\n",
    "        ax_vol.set_title(f'{title} - Annualized Volatility', fontsize=12)\n",
    "        ax_vol.set_xlabel('Assets')\n",
    "        ax_vol.set_ylabel('Volatility')\n",
    "        ax_vol.set_xticks(range(len(tickers)))\n",
    "        ax_vol.set_xticklabels(tickers, rotation=45, ha='right')\n",
    "        ax_vol.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Plot for each portfolio\n",
    "plot_returns_analysis(equities_features, axes[0, 0], axes[1, 0], 'Equities')\n",
    "plot_returns_analysis(fx_features, axes[0, 1], axes[1, 1], 'FX')\n",
    "plot_returns_analysis(futures_features, axes[0, 2], axes[1, 2], 'Futures')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/figures/returns_volatility_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… Returns and volatility analysis saved to reports/figures/returns_volatility_analysis.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d25e8a1",
   "metadata": {},
   "source": [
    "# Step 5: Store Cleaned Data for Speaicalist ML Training and Plots for Understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c404e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed data with features to disk AND ArcticDB\n",
    "import os\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs('../data/processed/equities', exist_ok=True)\n",
    "os.makedirs('../data/processed/fx', exist_ok=True)\n",
    "os.makedirs('../data/processed/futures', exist_ok=True)\n",
    "os.makedirs('../reports/figures', exist_ok=True)\n",
    "\n",
    "def save_portfolio_features(features_dict, portfolio_name, loader):\n",
    "    \"\"\"Save features for each ticker in a portfolio to CSV and ArcticDB.\"\"\"\n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(f\"SAVING {portfolio_name.upper()} PORTFOLIO DATA\")\n",
    "    print(f\"{'=' * 80}\")\n",
    "    \n",
    "    saved_count = 0\n",
    "    for ticker, df in features_dict.items():\n",
    "        try:\n",
    "            # Clean ticker name for filename\n",
    "            clean_ticker = ticker.replace('=', '_').replace('/', '_')\n",
    "            \n",
    "            # Save to CSV\n",
    "            filepath = f'../data/processed/{portfolio_name.lower()}/{clean_ticker}_features.csv'\n",
    "            df.to_csv(filepath)\n",
    "            print(f\"âœ… CSV: {ticker} -> {filepath}\")\n",
    "            \n",
    "            # Save to ArcticDB\n",
    "            library_name = f\"{portfolio_name.lower()}_features\"\n",
    "            loader.arctic_database.get_library(library_name, create_if_missing=True).write(\n",
    "                clean_ticker, df\n",
    "            )\n",
    "            print(f\"   ğŸ“Š ArcticDB: {ticker} -> {library_name}/{clean_ticker}\")\n",
    "            \n",
    "            saved_count += 1\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error saving {ticker}: {str(e)}\")\n",
    "    \n",
    "    print(f\"\\nğŸ“Š Saved {saved_count}/{len(features_dict)} tickers for {portfolio_name}\")\n",
    "    return saved_count\n",
    "\n",
    "# Save all portfolios to both CSV and ArcticDB\n",
    "total_saved = 0\n",
    "total_saved += save_portfolio_features(equities_features, 'equities', equities_loader)\n",
    "total_saved += save_portfolio_features(fx_features, 'fx', fx_loader)\n",
    "total_saved += save_portfolio_features(futures_features, 'futures', futures_loader)\n",
    "\n",
    "print(f\"\\n{'=' * 80}\")\n",
    "print(f\"TOTAL ASSETS SAVED: {total_saved}\")\n",
    "print(f\"{'=' * 80}\")\n",
    "print(\"\\nğŸ“ Data saved to:\")\n",
    "print(\"   CSV Files: data/processed/[equities|fx|futures]/\")\n",
    "print(\"\\nğŸ“Š ArcticDB Libraries Created:\")\n",
    "print(\"   - equities_data/equities_features\")\n",
    "print(\"   - fx_data/fx_features\")\n",
    "print(\"   - futures_data/futures_features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980ae98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive summary report\n",
    "print(\"=\" * 80)\n",
    "print(\"DATA LOADING AND EDA SUMMARY REPORT\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "summary = {\n",
    "    'Portfolio': ['Equities', 'FX', 'Futures', 'TOTAL'],\n",
    "    'Tickers Requested': [len(equities_tickers), len(fx_tickers), len(futures_tickers), \n",
    "                          len(equities_tickers) + len(fx_tickers) + len(futures_tickers)],\n",
    "    'Successfully Processed': [len(equities_features), len(fx_features), len(futures_features),\n",
    "                               len(equities_features) + len(fx_features) + len(futures_features)],\n",
    "    'Features per Asset': [\n",
    "        len(equities_features[list(equities_features.keys())[0]].columns) if equities_features else 0,\n",
    "        len(fx_features[list(fx_features.keys())[0]].columns) if fx_features else 0,\n",
    "        len(futures_features[list(futures_features.keys())[0]].columns) if futures_features else 0,\n",
    "        'Varies'\n",
    "    ]\n",
    "}\n",
    "\n",
    "summary_df = pd.DataFrame(summary)\n",
    "print(\"\\n\", summary_df.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SAVED FILES:\")\n",
    "print(\"=\" * 80)\n",
    "print(\"ğŸ“ Equities data: data/processed/equities/\")\n",
    "print(\"ğŸ“ FX data: data/processed/fx/\")\n",
    "print(\"ğŸ“ Futures data: data/processed/futures/\")\n",
    "print(\"ğŸ“Š Plots: reports/figures/\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"NEXT STEPS:\")\n",
    "print(\"=\" * 80)\n",
    "print(\"1. âœ… Data loaded and cleaned\")\n",
    "print(\"2. âœ… Features engineered\")\n",
    "print(\"3. âœ… EDA visualizations created\")\n",
    "print(\"4. âœ… Data saved for specialist training\")\n",
    "print(\"5. â­ï¸  Proceed to specialist environment testing (notebook 01)\")\n",
    "print(\"6. â­ï¸  Train specialist agents (notebook 02)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nğŸ‰ Notebook 00 Complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263bead2",
   "metadata": {},
   "source": [
    "## Bonus: Verify ArcticDB Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ad74e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify ArcticDB storage - list all libraries and symbols\n",
    "import arcticdb as adb\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ARCTICDB STORAGE VERIFICATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "portfolios = [\n",
    "    ('equities_data', equities_loader),\n",
    "    ('fx_data', fx_loader),\n",
    "    ('futures_data', futures_loader)\n",
    "]\n",
    "\n",
    "for db_name, loader in portfolios:\n",
    "    print(f\"\\nğŸ“Š Database: {db_name}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # List all libraries in this database\n",
    "    try:\n",
    "        libraries = loader.arctic_database.list_libraries()\n",
    "        print(f\"Libraries: {libraries}\")\n",
    "        \n",
    "        for lib_name in libraries:\n",
    "            lib = loader.arctic_database.get_library(lib_name)\n",
    "            symbols = lib.list_symbols()\n",
    "            print(f\"\\n  Library '{lib_name}': {len(symbols)} symbols\")\n",
    "            if symbols:\n",
    "                print(f\"    Sample symbols: {symbols[:5]}\")\n",
    "                \n",
    "                # Show info for first symbol\n",
    "                if symbols:\n",
    "                    sample_data = lib.read(symbols[0]).data\n",
    "                    print(f\"    Sample data shape for '{symbols[0]}': {sample_data.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  Error: {str(e)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"âœ… ArcticDB verification complete!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3327da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: How to load data back from ArcticDB\n",
    "print(\"=\" * 80)\n",
    "print(\"EXAMPLE: LOADING DATA FROM ARCTICDB\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Load raw market data\n",
    "print(\"\\n1ï¸âƒ£ Loading raw market data from equities_data database:\")\n",
    "equities_loader.display_data(\"raw_market_data\", symbol=\"data\", head=5)\n",
    "\n",
    "# Load processed features for a specific ticker\n",
    "print(\"\\n2ï¸âƒ£ Loading processed features from equities_features library:\")\n",
    "if equities_features:\n",
    "    sample_ticker = list(equities_features.keys())[0]\n",
    "    clean_ticker = sample_ticker.replace('=', '_').replace('/', '_')\n",
    "    \n",
    "    lib = equities_loader.arctic_database.get_library('equities_features')\n",
    "    loaded_features = lib.read(clean_ticker).data\n",
    "    \n",
    "    print(f\"\\nLoaded {clean_ticker} features from ArcticDB:\")\n",
    "    print(f\"Shape: {loaded_features.shape}\")\n",
    "    print(f\"Columns: {loaded_features.columns.tolist()[:10]} ...\")\n",
    "    print(f\"\\nFirst 3 rows:\")\n",
    "    print(loaded_features.head(3))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"âœ… Data retrieval example complete!\")\n",
    "print(\"\\nğŸ’¡ TIP: You can now access all your data from ArcticDB in future notebooks!\")\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hrl_fund",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
